{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코드 파트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blog_crawling(start_day, end_day, keyword):\n",
    "    URLlist = []\n",
    "    first_url = ''\n",
    "    for j in range(100):\n",
    "        param = {\n",
    "            \"where\" : \"post\",\n",
    "            \"query\" : keyword,\n",
    "            \"date_from\" : start_day,\n",
    "            \"date_to\" : end_day,\n",
    "            \"date_option\" : \"8\",\n",
    "            \"start\" : str(10*j+1) \n",
    "        }\n",
    "        response = requests.get(url,params=param, headers=hrd)\n",
    "        print(\"--------\",response.url,\"-----------\")\n",
    "        f = open(\"C:/Users/Joobe/Downloads/NaverBlog_{0:}.txt\".format(keyword), \"a\", encoding=\"utf-8\")\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for links in soup.select(\"li.sh_blog_top > dl\"):\n",
    "            try:\n",
    "                title_html = links.select(\"dt>a\")\n",
    "                content_html = links.select(\"dd.sh_blog_passage\")\n",
    "                author_url_html = links.select(\"dd.txt_block a\")\n",
    "                title = title_html[0].get('title')\n",
    "                content = content_html[0].text\n",
    "                author = author_url_html[0].text\n",
    "                URL = author_url_html[1].get('href')\n",
    "                URL = get_final_url(URL)\n",
    "                if URL == first_url:\n",
    "                    print(\"중복. 다음 기간으로 넘어갑니다.\")\n",
    "                    return\n",
    "                if URL ==\"none\": continue\n",
    "                URLlist.append(URL)\n",
    "                res = req.urlopen(URL)\n",
    "                soup = BeautifulSoup(res, 'html.parser')\n",
    "                cont = soup.find(\"div\",{\"class\":\"se-main-container\"})\n",
    "                content = cont.get_text()\n",
    "            except:\n",
    "                try: \n",
    "                    cont = soup.find(\"div\",{\"id\":\"postViewArea\"})\n",
    "                    content = cont.get_text()\n",
    "                except:\n",
    "                    try:\n",
    "                        content = ' '.join([x.get_text() for x in soup.findAll(\"p\",{\"class\":\"se_textarea\"})])\n",
    "                    except:\n",
    "                        print(\"\\n\\n\" + URL + \"예외처리발생\\n\\n\")\n",
    "                        return\n",
    "            finally:\n",
    "                filtered_content = re.sub(\"[\\n\\u200b]\",\" \",content)\n",
    "                filtered_content = re.sub(\"\\s+\", \" \", filtered_content)\n",
    "            try: \n",
    "                f.write(filtered_content+\"\\n\")\n",
    "            except:\n",
    "                print(\"파일에작성되지않음\")\n",
    "            #print(keyword, response.url)\n",
    "            #blog_crawling(keyword, response.url)\n",
    "            #print(response.text)\n",
    "            #print(type(response.url))\n",
    "        if URLlist:\n",
    "            first_url = URLlist[0]\n",
    "        else:\n",
    "            return\n",
    "        URLlist.clear()\n",
    "        f.close()\n",
    "        #if j==2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://search.naver.com/search.naver\"\n",
    "hrd = {\"User-Agent\":\"Mozilla/5.0\",\"referer\":\"http://naver.com\"}\n",
    "start_days = pd.date_range(start='20160101', end='20190731', freq=\"MS\")\n",
    "end_days = pd.date_range(start=\"20160101\",end=\"20190731\",freq=\"M\")\n",
    "\n",
    "# 2016.01 ~ 2019.07 동안 총 43달 == len(start_days)\n",
    "\n",
    "def NaverBlogCrawl(keyword):\n",
    "    for i in range(len(start_days)):\n",
    "        print(\"**새로운 기간 시작**\")\n",
    "        start_day = start_days[i].strftime(\"%Y%m%d\")\n",
    "        end_day = end_days[i].strftime(\"%Y%m%d\")\n",
    "        blog_crawling(start_day, end_day, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실행 파트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**새로운 기간 시작**\n",
      "-------- https://search.naver.com/search.naver?where=post&query=SOFEX&date_from=20160101&date_to=20160131&date_option=8&start=1 -----------\n"
     ]
    }
   ],
   "source": [
    "NaverBlogCrawl(\"장호항\") # NaverBlogCrawl(\"검색어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
